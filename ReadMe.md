# Project Overview
This project aims to evaluate the practical effectiveness of AI tools in Go language development. Using the Anscombe Quartet dataset as a foundation, we developed three versions of Go programs: a manually written version, an optimized version generated by GitHub Copilot, and a version directly generated by Gemini 2.5 Pro. We compared the three in terms of execution efficiency, result accuracy, and code clarity to provide empirical insights into the real-world value of AI tools in software projects.

---

# Code Comparison
The version I wrote by hand has a simple structure and clear logic, and I’m familiar with the variable names I chose. Compared to the version optimized by GitHub Copilot, the latter introduced sync.WaitGroup and goroutines to implement multithreaded regression computations. It also added some error handling, making it more like an intelligent auto-completion tool with enhancement suggestions.

The version generated by Gemini 2.5 Pro has the most complete and modular structure. It encapsulated functions such as readCSVData, groupDataByDataset, and calculateLinearRegression, and ensured consistent output order through sorting, which improved readability and user experience. Error handling was also thorough, with detailed messages for invalid records, empty files, and field errors.

However, when I asked Gemini to generate code, it attempted to call a LinearRegression() function, which does not actually exist in the [github.com/montanaflynn/stats](https://github.com/montanaflynn/stats). I eventually had to manually change it to the simpleLinearRegression() function. This issue highlights a common limitation of Gemini or other AI assistants: they do not truly “know” whether a specific function exists in a third-party package—they generate code based on language model “experience.” Therefore, the output often still requires human validation and correction.

In summary, each approach has its strengths. Handwriting code is better for understanding the logic, Copilot assistance improves efficiency, and Gemini provides greater automation. Overall, AI tools can serve as powerful assistants in programming, but human developers are still needed to guide and oversee the process.

---

# Regression Results Comparison
Regression results are as follows:

Automated Code & GitHub Copilot Code:
Set I: Slope = 0.5001, Intercept = 3.0001

Set II: Slope = 0.5000, Intercept = 3.0009

Set III: Slope = 0.4997, Intercept = 3.0025

Set IV: Slope = 0.4999, Intercept = 3.0017

Gemini 2.5 Pro Code:
Set I: Slope = 0.500, Intercept = 3.000

Set II: Slope = 0.500, Intercept = 3.001

Set III: Slope = 0.500, Intercept = 3.002

Set IV: Slope = 0.500, Intercept = 3.002

---

# Benchmark Results Comparison
Benchmark results are as follows:

Automated Code:
B/op: 21,656
allocs/op: 177
Time：2.534s

GitHub Copilot Code:
B/op: 24,385
allocs/op: 180
Time：1.579s

Gemini 2.5 Pro Code:
B/op: 21,656
allocs/op: 177
Time：1.459s

---

# Test Results Summary

My original code and Copilot's code produced the most accurate results. The slopes and intercepts of the four datasets matched the theoretical values of the Anscombe dataset exactly. The code generated by Gemini showed slight deviations in either slope or intercept, making them less precise than the original version.

However, my code took the longest to run, with an execution time of 2.534 seconds. Both Copilot's and Gemini's versions were a full second faster. While Copilot's code reduced execution time, it came at the cost of higher memory usage. In contrast, Gemini's code used the same amount of memory as mine. From this perspective, Gemini's implementation performed the best.

---

# Recommendation to Management
Based on the comparative testing results of this project, we recommend that this startup selectively adopt AI-assisted programming tools in future software development, but avoid full reliance on them.

AI-generated code takes only a few seconds to produce, which can significantly boost development efficiency. However, the generated code is not guaranteed to be entirely correct or aligned with human expectations. This highlights that AI-generated code still requires human review to ensure logical rigor and result accuracy.
Therefore, we suggest using AI tools as development aid. For example, to speed up code prototyping, generate test functions, or offer refactoring suggestions, while the implementation of core business logic should remain in the hands of experienced engineers. At the same time, to ensure code quality and security, a mechanism for manual review and unit testing must be established as a safeguard.

Overall, AI tools can help improve development efficiency and reduce certain labor costs, serving as a productivity booster for the team. However, they are not recommended as a full replacement for professional programmers.

---

# Training Materials

- [GitHub Copilot] (https://github.com/features/copilot)
- [Gemini 2.5 Pro] (https://gemini.google.com)

